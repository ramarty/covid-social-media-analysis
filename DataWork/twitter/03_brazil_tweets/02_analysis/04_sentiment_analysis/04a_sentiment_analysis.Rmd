---
title: "COVID Webscraping and Social Media Analysis: Twitter Text Mining"
author: Rony Rodriguez-Ramirez
output:
  tufte::tufte_handout:
    keep_tex: true
  tufte::tufte_html: default
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

```{r fig-margin-separate, message = FALSE, include = FALSE, cache = TRUE}
# Packages ---------------------------------------------------------------------
if (!require("pacman")) install.packages("pacman")

pacman::p_load(gtrendsR, tidyverse, parallel, pbmcapply, ggplot2, scales,
               widyr, ggraph, igraph, jsonlite, stringr, raster, stringi, 
               lubridate, purrr, lexiconPT, tidytext, quanteda, qdap, 
               SentimentAnalysis, sentimentr, patchwork, tm, tokenizers, 
               wordcloud, ggwordcloud, ggpubr, hrbrthemes)

# Folder settings --------------------------------------------------------------
if (Sys.getenv("USERNAME") == "maximiliano") {
    dropbox_file_path       <- file.path("D:/Dropbox/COVID Social Media Analysis")
    github_file_path        <- file.path("D:/Documents/RA Jobs/DIME/github/covid-social-media-analysis")
    covid_twitter_github    <- file.path("D:/Documents/RA Jobs/DIME/github/COVID-19-TweetIDs")
}

brazil_twitter_figures_path <- file.path(dropbox_file_path, "Data", "twitter", "Outputs", "figures")

```

# Introduction

This note describes the use of Twitter to analyze conversations, opinions and sentiment
surrounding the COVID-19 pandemic in Brazil. We draw from a public Twitter dataset
that scrapes coronavirus related tweets in real time starting from the end of January.
The dataset contains over 70 million tweets. We restrict tweets to those that contained
a Brazil location in the tweet or in the user’s location description.
This process yields about 1,090,400 tweets for Brazil. Excluding retweets, the total number of
tweets is about 330,650.

```{r message = FALSE, include = FALSE, cache = TRUE}
# 1. Load Data --------------------------------------------------------------------
brazil_tweets <- readRDS(file.path(dropbox_file_path, "Data", "twitter", "FinalData", "brazil_tweets", "rds", "brazil_tweets_appended_clean.Rds"))

# 2. Tweets Over Time -------------------------------------------------------------

    #### 2.1. Daily sum: All Tweets 
    brazil_tweets_daysum <- brazil_tweets %>%
        filter(constant_words %in% T) %>%
        mutate(
            # Get an indicator for the quantity of RTs and No RTs per day
            rts = ifelse(grepl("rt @", full_text), "RT", "No RT") 
        ) %>% 
        group_by(date, rts) %>%
        summarise(N = n(),
                  N_corona = sum(grepl("corona", full_text)),
                  N_coronavirus = sum(grepl("coronavirus", full_text)),
                  N_covid = sum(grepl("covid", full_text)), 
                  N_hospital = sum(grepl("hospital", full_text)),
                  N_socialdistance = sum(grepl("distância social", full_text)),
                  N_mort = sum(grepl("mort", full_text)),
                  N_quarentena = sum(grepl("quarentena", full_text)))
    
    #### 2.2. SUM Words - Long version - All Tweets
    brazil_tweets_daysum_long <- brazil_tweets_daysum %>%
        dplyr::select(-N) %>% 
        pivot_longer(
            cols = starts_with("N_"),
            names_to = "word",
            values_to = "count"
        ) %>% 
        mutate(
            word = gsub("N_", "", word),
            word = str_to_title(word)
        )
    
    #### 2.3 Get only text from the tweets 
    tweets_text <- brazil_tweets %>% 
        mutate(
            tweet_id = row_number()
        ) %>% 
        dplyr::select(tweet_id, full_text, date)
    
    #### 2.4 Eliminate stop words
    stopwords_multilang <- c(stopwords::stopwords("en"),
                             stopwords::stopwords("pt"),
                             stopwords::stopwords("es"),
                             "t.co")
    
    stopwords_multilang_df <- as.data.frame(stopwords_multilang) %>% 
        janitor::clean_names() %>% 
        rename(word = stopwords_multilang) %>% 
        mutate(
            word = as.character(word)
        )
    
    #### 2.5 Unnest tweets: Words per tweets
    unnest_words <- tweets_text %>%
        unnest_tokens(word, full_text) %>% 
        anti_join(stop_words, by = "word") %>% 
        anti_join(stopwords_multilang_df, by = "word") %>% 
        filter(!word %in% c("rt", "pra", "é", "tá", "sjgtzxmbpv", "vinistupido", "ta", 
                            "vcs", "pq", "aí", "pq", "paulo")) 
    
    #### 2.6. Top words for the whole period
    top_words <- unnest_words %>%
        count(word, sort = TRUE) %>%
        mutate(word = fct_reorder(word, n)) %>%
        head(20)
    
    #### 2.7 Correlation across words
    words_filtered <- unnest_words %>%
        dplyr::select(-date) %>%
        add_count(word) %>%
        filter(n >= 10000)
    
    occurences <- words_filtered %>%
        group_by(word) %>%
        summarize(occurences = n())
    
    top_word_cors  <- words_filtered %>% 
        pairwise_cor(word, tweet_id, sort = TRUE) %>%
        head(175)
    
    vertices <- occurences %>%
        filter(word %in% top_word_cors$item1 |
                   word %in% top_word_cors$item2)
    
    #### 2.8 Sentiments 
    ## Load PT lexicon from the lexiconPT Package
    data("sentiLex_lem_PT02") 
    
    ## Combine portuguese and english words 
    sentiments <- get_sentiments("nrc") %>% 
        mutate(term = word) %>% 
        bind_rows(sentiLex_lem_PT02) %>% 
        mutate(sentiment = case_when(polarity == -1 ~ "negative",
                                     polarity == 1 ~ "positive",
                                     polarity == 0 ~ "neutral",
                                     TRUE ~ as.character(sentiment)),
               word = ifelse(is.na(word), term, word)
        ) %>% 
        dplyr::select(word, sentiment) %>% 
        filter(sentiment %in% c("positive", "negative")) %>% 
        mutate(
            sentiment = str_to_title(sentiment)
        )
            
    ## Join with words that include sentiments
    tweets_sentiments <- unnest_words %>%
        right_join(sentiments) %>% 
        filter(!is.na(date)) 
    
    ## Most common positive and negative words
    sentiments_counts <- tweets_sentiments %>% 
        count(word, sentiment, sort = TRUE) %>% 
        ungroup()  
    
    #### 2.9 HASHTAGS
    ## List of hashtags
    hashtags <- c("fiqueemcasa",
                  "covid19",
                  "coronavirus",
                  "vaipassar",
                  "distanciasalva",
                  "euficoemcasa",
                  "isolamentosocial",
                  "sus",
                  "profissionaisdesaúde",
                  "medicos",
                  "medicas",
                  "enfermeiros", 
                  "enfermeiras",
                  "mascara")
    
    ## Filter tweets with a specific hashtag
    ht_dfs <- data.frame()
    
    for(term in hashtags){
        print(term)
        
        ht_df <- tweets_text %>% 
            filter(grepl(term, full_text)) %>% 
            unnest_tokens(word, full_text) %>% 
            anti_join(stop_words, by = "word") %>% 
            anti_join(stopwords_multilang_df, by = "word") %>% 
            filter(!word %in% c("rt", "pra", "é", "tá", "sjgtzxmbpv", "vinistupido", "ta", 
                                "vcs", "pq", "aí", "pq", "paulo")) %>% 
            right_join(sentiments) %>% 
            filter(!is.na(date)) %>% 
            mutate(
                variable = paste0(term)
            )
        
        ht_dfs <- bind_rows(ht_dfs, ht_df)
    }
    
    #### 2.10 TOP WORDS
    grab_phases_for_wordcloud <- function(brazil_tweets){
        # Function that grabs phrases for a word cloud.
        
        brazil_tweets_t <- brazil_tweets %>%
            filter(!grepl("rt @", full_text))
        
        words <- brazil_tweets_t$full_text %>% tokenize_words(stopwords = stopwords_multilang)
        ngram2 <- brazil_tweets_t$full_text %>% tokenize_ngrams(n=2, stopwords = stopwords_multilang)
        ngram3 <- brazil_tweets_t$full_text %>% tokenize_ngrams(n=3, stopwords = stopwords_multilang)
        
        phrases <- c(
            words %>% unlist(),
            ngram2 %>% unlist(),
            ngram3 %>% unlist()) %>%
            table() %>%
            as.data.frame() %>%
            dplyr::rename(word = ".",
                          freq = Freq) %>%
            mutate(word = word %>% as.character())
        
        phrases <- phrases %>%
            filter(freq >= 10) %>%
            filter(word != "t.co") %>%
            filter(!grepl("corona|china|virus|covid|virus|brasil", word)) %>%
            filter(nchar(word) >= 4) %>%
            arrange(desc(freq))
        
        return(phrases)
    }
    
    phrases_all_tweets  <- grab_phases_for_wordcloud(brazil_tweets)
    
    phrases_quarentena  <- grab_phases_for_wordcloud(brazil_tweets %>% filter(grepl("quarentena", full_text))) %>% filter(word != "quarentena")
    phrases_coronavirus <- grab_phases_for_wordcloud(brazil_tweets %>% filter(grepl("coronavirus", full_text)))   %>% filter(word != "coronavirus")
    phrases_hospital    <- grab_phases_for_wordcloud(brazil_tweets %>% filter(grepl("hospital", full_text)))   %>% filter(word != "hospital")
    phrases_carnaval    <- grab_phases_for_wordcloud(brazil_tweets %>% filter(grepl("carnaval", full_text)))   %>% filter(word != "carnaval")
    phrases_cancelar    <- grab_phases_for_wordcloud(brazil_tweets %>% filter(grepl("cancelar", full_text)))   %>% filter(word != "cancelar")
    phrases_bolsonaro   <- grab_phases_for_wordcloud(brazil_tweets %>% filter(grepl("bolsonaro", full_text)))  %>% filter(!grepl("bolsonaro", word))


    # GGPLOT: Top words when said: 
    phrase_plot <- function(data, title){
        
        data %>% 
            filter(word != "vírus") %>% 
            head(10) %>% 
            ggplot(aes(x = fct_reorder(word,freq), y = freq)) +
            geom_col(fill = "darkslategrey", color = "black") +
            coord_flip() + 
            labs(
                x = NULL, 
                y = "Number of references", 
                title = title
            ) +
            theme_ipsum_rc() + 
            theme(
                panel.grid.minor = element_blank(),
                legend.position = "none"
            )
    }
    
    #### 2.11 Bigram
    tweets_bigrams <- tweets_text %>%
        unnest_tokens(word, full_text, token = "ngrams", n = 2)
    
    tweets_bigrams_separated <- tweets_bigrams %>% 
        separate(word, c("word1", "word2"), sep = " ")
    
    
    tweets_bigrams_filtered <- tweets_bigrams_separated %>% 
        filter(!word1 %in% stop_words$word) %>%
        filter(!word2 %in% stop_words$word) %>% 
        filter(!word1 %in% stopwords_multilang_df$word) %>%
        filter(!word2 %in% stopwords_multilang_df$word) %>% 
        filter(!word1 %in% c("rt", "pra", "é", "tá", "sjgtzxmbpv", "vinistupido", "ta", 
                            "vcs", "pq", "aí", "pq", "paulo")) %>% 
        filter(!word2 %in% c("rt", "pra", "é", "tá", "sjgtzxmbpv", "vinistupido", "ta", 
                            "vcs", "pq", "aí", "pq", "paulo")) 
    
    tweets_bigrams_united <- tweets_bigrams_filtered %>% 
        unite(bigram, word1, word2, sep = " ")
    
    ## CORONA: Words preceded by: something
    corona_words <- tweets_bigrams_separated %>% 
        filter(word1 == "corona",
               !word2 %in% c("virus", "beer")) %>% 
        inner_join(sentiments, by = c(word2 = "word")) %>% 
        count(word2, sentiment, sort = TRUE)
    
    ## QUARENTENA, HOSPITAL: Words preceded by: something
    preceded_words <- tweets_bigrams_separated %>% 
        filter(word1 %in% c("quarentena", "hospital"), 
               !word2 %in% c("virus", "beer")) %>% 
        inner_join(sentiments, by = c(word2 = "word")) %>% 
        count(word1, word2, sentiment, sort = TRUE)
    
```

# Overall Trends

First, we plot the overall trend for a selected group of words which are: coronavirus, corona, hospital, and quarentena.
Additionally, we plot in a separate figure the word "covid" given that the total number of tweets that include this word is significantly higher that the first four presented above.

```{r fig.fullwidth = TRUE, fig.width = 9, fig.height = 6, fig.show='hold', cache=TRUE, message=FALSE, echo=FALSE}
#### 3.1.1 Combined rts and no rts summary
        brazil_tweets_daysum_long %>% 
            filter(!word %in% c("Covid", "Socialdistance", "Mort")) %>%
            group_by(rts) %>% 
            ggplot(aes(x = date, y = count, color = rts)) + 
            geom_line(size = 1) + 
            facet_wrap(~word) +
            scale_y_continuous(label = comma) + 
            labs(
                x = NULL, 
                y = "# of Tweets",
                color = NULL,
                title = "Tweets per day in Brazil that include the word:",
                subtitle = "Selected words"
            ) + 
            theme_ipsum_rc() + 
            theme(
                panel.grid.minor = element_blank(),
            )

        #### 3.2.1 
        brazil_tweets_daysum_long %>% 
            filter(word == "Covid") %>%
            ggplot(aes(x = date, y = count, color = rts)) + 
            geom_line(size = 1) + 
            scale_y_continuous(label = comma) + 
            labs(
                x = NULL, 
                y = "# of Tweets",
                color = NULL,
                title = "Tweets that mention the word COVID in Brazil"
            ) + 
            theme_ipsum_rc() + 
            theme(
                panel.grid.minor = element_blank()
            )
```

# Text Mining

## Top words

Since the dataset we are using contains only tweets related to the coronavirus outbreak, we expect to see words related to it as the commond words used in tweets from Brazil.

```{r fig.fullwidth = TRUE, fig.width = 9, fig.height = 6, fig.show='hold', cache=TRUE, message=FALSE, echo=FALSE}
    # 4.5 Graph: common words
    top_words %>% 
        ggplot(aes(word, n)) +
        geom_col() +
        coord_flip() +
        scale_y_continuous(label = comma,
                           breaks = seq(0, 250000, by = 50000),
                           expand = c(0,0)) + 
            labs(
                x = NULL, 
                y = "Count",
                color = NULL,
                title = "Common words in Tweets in Brazil",
                subtitle = "From January 21st, 2020 to April 3rd, 2020"
            ) + 
            theme_ipsum_rc() + 
            theme(
                panel.grid.minor = element_blank()
            )
```
We may be interested in visualizing the relationships among words. We can arrange the words into a network of relationship which is a combination of connected nodes. The figure below presents clusters of words that appear the most in the tweets from Brazil. 

```{r fig.fullwidth = TRUE, fig.width = 9, fig.height = 6, fig.show='hold', cache=TRUE, message=FALSE, echo=FALSE}
    top_word_cors %>%
        graph_from_data_frame(vertices = vertices) %>%
        ggraph(layout = "fr") +
        geom_edge_link() +
        geom_node_point(aes(size = occurences)) +
        geom_node_text(aes(label = name, 
                           family = "Roboto Condensed"), 
                       repel = TRUE) +
        scale_size_continuous(label = comma) + 
        labs(
            x = NULL,
            y = NULL,
            title = "Cluster of words that appear the most in tweets from Brazil",
            subtitle = "Figure only includes words with more than 10,000 mentions",
            size = "# of occurrences"
        ) +
        theme_ipsum_rc() +
        theme(
            legend.position = "bottom",
            axis.ticks = element_blank(),
            axis.text.x = element_blank(),
            axis.text.y = element_blank(),
            panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(),
            panel.border = element_blank()
        )
```

We also present the top 10 word pairs for the whole period of the dataset using pairwise correlation (i.e., how often two words appear together). 

```{r fig.fullwidth = TRUE, fig.width = 9, fig.height = 6, fig.show='hold', cache=TRUE, message=FALSE, echo=FALSE}
    top_word_cors %>% 
        distinct(correlation, .keep_all = TRUE) %>% 
        unite("pair", item1, item2, sep = "-") %>% 
        mutate(pair = fct_reorder(pair, correlation)) %>% 
        head(10) %>% 
        ggplot(aes(x = correlation, y = pair)) + 
        geom_col() + 
        scale_x_continuous(limits = c(0,1),
                           expand = c(0,0)) +
        labs(
            y = "Pair of words", 
            x = "Correlation",
            title = "Top 10 word pairs with the highest correlation in Tweets of Brazil"
        ) + 
        theme_ipsum_rc() + 
        theme(
            panel.grid.minor = element_blank()
        )
```

## Sentiment Analysis

In terms of sentiment analysis, we make use of the Portuguese and English lexicons and assign "positive" and "negative" value to each word within a tweet. 


```{r fig.fullwidth = TRUE, fig.width = 9, fig.height = 6, fig.show='hold', cache=TRUE, message=FALSE, echo=FALSE}
sentiments_counts %>% 
        group_by(sentiment) %>% 
        top_n(10) %>% 
        ungroup() %>% 
        mutate(word = reorder(word, n)) %>% 
        ggplot(aes(word, n, fill = sentiment)) + 
        geom_col(show.legend = FALSE) + 
        coord_flip() + 
        facet_wrap(~sentiment, scales = "free_y") + 
        scale_y_continuous(label = comma) + 
        labs(
            x = "", 
            y = "Contribution to sentiment",
            title = "Words that contribute to positive and negative sentiment in Brazil"
        ) + 
        theme_ipsum_rc() + 
        theme(
            panel.grid.minor = element_blank(),
            legend.position = "bottom"
        )

## No "Virus" word
    sentiments_counts %>% 
        filter(word != "virus") %>% 
        group_by(sentiment) %>% 
        top_n(10) %>% 
        ungroup() %>% 
        mutate(word = reorder(word, n)) %>% 
        ggplot(aes(word, n, fill = sentiment)) + 
        geom_col(show.legend = FALSE) + 
        coord_flip() + 
        facet_wrap(~sentiment, scales = "free_y") + 
        scale_y_continuous(label = comma) + 
        labs(
            x = "", 
            y = "Contribution to sentiment",
            title = "Words that contribute to positive and negative sentiment in Brazil",
            subtitle = "Excluding the word virus"
        ) + 
        theme_ipsum_rc() + 
        theme(
            panel.grid.minor = element_blank(),
            legend.position = "bottom"
        )
            
```

We select three of the most common hashtags used in Brazil which are: (1) #coronavirus, (2) #covid19, and (3) #sus, and explore the sentiments in the tweets that contained each of these hashtags.^[Please note that the current state of the dataset does not allow us to explore hashtags fully.]

```{r fig.fullwidth = TRUE, fig.width = 10, fig.height = 6, fig.show='hold', cache=TRUE, message=FALSE, echo=FALSE}
    # Grouped GGPLOT
    ht_dfs %>% 
        filter(variable %in% c("coronavirus", "covid19", "sus")) %>%
        mutate(
            variable = paste0("#", variable)
        ) %>% 
        group_by(date, variable) %>% 
        count(date, sentiment) %>% 
        ggplot(aes(x = date, y = n, fill = sentiment)) + 
        geom_bar(stat = "identity", width = 1) +
        scale_y_continuous(label = comma) + 
        facet_wrap(~variable) + 
        labs(
            x = "", 
            y = "Number of tweets",
            fill = "Sentiment", 
            title = "Tweets per day with negative and positive sentiments in Brazil that included the HT:"
        ) + 
        theme_ipsum_rc() + 
        theme(
            panel.grid.minor = element_blank(),
            legend.position = "bottom"
        )
```

Additionally, we can also explore that other words were mentioned when a specific word was included in the tweet.

```{r fig.fullwidth = TRUE, fig.width = 15, fig.height = 10, fig.show='hold', cache=TRUE, message=FALSE, echo=FALSE}
    quarentena  <- phrase_plot(phrases_quarentena,  "quarentena")
    coronavirus <- phrase_plot(phrases_coronavirus, "coronavirus")
    hospital    <- phrase_plot(phrases_hospital,    "hospital")
    carnaval    <- phrase_plot(phrases_carnaval,    "carnaval")
    cancelar    <- phrase_plot(phrases_cancelar,    "cancelar")
    bolsonaro   <- phrase_plot(phrases_bolsonaro,   "Bolsonaro")
    
    quarentena + coronavirus + hospital + bolsonaro + carnaval + cancelar +
        plot_annotation(
            title = 'Top words mentioned when tweets include the word:', 
            theme = theme(
                        plot.title = element_text(size = 20, family = "Roboto Condensed", face = "bold")
                    )
        )
``` 
## Relationship between words: bigrams

Finally, we also perfom a sentiment analysis on the bigram data (i.e., relationship between a word and its preceding word). We examine how often sentiment-associated words are preceded by the word "corona", "hospital", and "quarentena". 

```{r fig.fullwidth = TRUE, fig.width = 9, fig.height = 6, fig.show='hold', cache=TRUE, message=FALSE, echo=FALSE}
corona_words %>% 
        filter(!word2 %in% c("lover")) %>% 
        mutate(value = ifelse(sentiment == "Negative", -1, 1),
               contribution = n * value) %>% 
        arrange(desc(abs(contribution))) %>% 
        head(30) %>% 
        mutate(word2 = reorder(word2, contribution)) %>% 
        ggplot(aes(x = word2, y = n * value, fill = sentiment)) +
        geom_col(show.legend = FALSE) + 
        coord_flip() + 
        labs(
            x = NULL, 
            y = "Sentiment value times number of occurences",
            fill = "Sentiment", 
            title = "Words preceded by \"corona\""
        ) + 
        theme_ipsum_rc() + 
        theme(
            panel.grid.minor = element_blank(),
            legend.position = "bottom"
        )

  preceded_words %>% 
          group_by(word1) %>% 
          top_n(15) %>% 
          mutate(value = ifelse(sentiment == "Negative", -1, 1),
                 contribution = n * value) %>% 
          ungroup() %>% 
          mutate(word2 = reorder_within(word2, contribution, word1),
                 word1 = str_to_title(word1)) %>% 
          ggplot(aes(x = word2, y = n * value, fill = sentiment)) +
          geom_col(show.legend = FALSE) + 
          coord_flip() + 
          scale_x_reordered() +
          scale_y_continuous(expand = c(0,0)) +
          facet_wrap(~word1, scales = "free_y") + 
          labs(
              x = NULL, 
              y = "Sentiment value times number of occurences",
              fill = "Sentiment", 
              title = "Top 15 words preceded by:"
          ) + 
          theme_ipsum_rc() + 
          theme(
              panel.grid.minor = element_blank(),
              strip.text = element_text(face = "bold"),
              legend.position = "bottom",
  
          )

``` 

# Appendix

Current problems with the dataset.^[For a detailed explanation of the issues with the dataset used in this document please visit [the COVID-19-TweetIDs repository]( https://github.com/echen102/COVID-19-TweetIDs/)]

## Known Gaps

| Date          | Time              |
|-------------  |-----              |
| 2/1/2020      | 4:00 - 9:00 UTC   |
| 2/8/2020      | 6:00 - 7:00 UTC   |
| 2/22/2020     | 21:00 - 24:00 UTC |
| 2/23/2020     | 0:00 - 24:00 UTC  |
| 2/24/2020     | 0:00 - 4:00 UTC   |
| 2/25/2020     | 0:00 - 3:00 UTC   |
| 3/2/2020      | Intermittent Internet Connectivity Issues |


